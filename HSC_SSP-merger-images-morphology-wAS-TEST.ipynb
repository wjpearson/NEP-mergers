{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "from tf_fits.image import image_decode_fits\n",
    "from tf_fits.bintable import bintable_decode_fits\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if GPUs. If there are, some code to fix cuDNN bugs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = './valid/'\n",
    "test_path = './test/'\n",
    "\n",
    "extn = '.fits'\n",
    "\n",
    "CLASS_NAMES = ['merger', 'nonmerger']\n",
    "NO_CLASS = len(CLASS_NAMES)\n",
    "\n",
    "valid_images = glob.glob(valid_path+'**/*'+extn)\n",
    "valid_image_count = len(valid_images)\n",
    "test_images = glob.glob(test_path+'**/*'+extn)\n",
    "test_image_count = len(test_images)\n",
    "\n",
    "EPOCHS = 1\n",
    "VALID_BATCH_SIZE = valid_image_count\n",
    "TEST_BATCH_SIZE = test_image_count\n",
    "\n",
    "STEPS_PER_VALID_EPOCH = 1\n",
    "STEPS_PER_TEST_EPOCH = 1\n",
    "\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "edge_cut = (128 - IMG_HEIGHT)//2\n",
    "CROP_FRAC = IMG_HEIGHT/(edge_cut+edge_cut+IMG_HEIGHT)\n",
    "\n",
    "print(valid_image_count, STEPS_PER_VALID_EPOCH)\n",
    "print(test_image_count, STEPS_PER_TEST_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_columns(table):\n",
    "    columns = tf.gather(table, [1,2,3,4,5,6,7,10,11,12,13,14,15,30,31,32,41])#,51,52])\n",
    "    return columns\n",
    "\n",
    "@tf.function\n",
    "def _normalise_condition(i, numcol, tf_MIN_MAX, table, new_table):\n",
    "    return tf.math.less(i, numcol)\n",
    "\n",
    "@tf.function\n",
    "def _normalise_body(i, numcol, tf_MIN_MAX, table, new_table):\n",
    "    \n",
    "    MIN = tf.gather_nd(tf_MIN_MAX, [i,0], name='MIN_slice')\n",
    "    MAX = tf.gather_nd(tf_MIN_MAX, [i,1], name='MAX_slice')\n",
    "    \n",
    "    column = tf.gather_nd(table, [i], name='table_gather')\n",
    "    column -= MIN\n",
    "    column /= tf.subtract(MAX, MIN)\n",
    "    new_table = new_table.write(i, column)\n",
    "    \n",
    "    i += 1\n",
    "    return i, numcol, tf_MIN_MAX, table, new_table\n",
    "\n",
    "@tf.function\n",
    "def normalise_columns(table):\n",
    "    tf_MIN_MAX = tf.constant([[-4.0, 4.0],   #asymmetry\n",
    "                              [0.0, 6.0],    #concentration\n",
    "                              [0.0, 3.0],    #deviation\n",
    "                              [0.0, 1.0],    #ellipticity_asymmetry\n",
    "                              [0.0, 1.0],    #ellipticity_centroid\n",
    "                              [1.0, 8.0],    #elongation_asymmetry\n",
    "                              [1.0, 8.0],    #elongation_centroid\n",
    "                              [0.0, 1.0],    #gini\n",
    "                              [-3.0, 3.0],   #gini_m20_bulge\n",
    "                              [-1.0, 1.0],   #gini_m20_merger\n",
    "                              [0.0, 1.0],    #intensity\n",
    "                              [-4.0, 0.0],   #m20\n",
    "                              [0.0, 1.0],    #multimode\n",
    "                              [0.0, 200.0],  #sersic_amplitude\n",
    "                              [-6.0, 3.0],   #sersic_ellip\n",
    "                              [0.0, 50.0],   #sersic_n\n",
    "                              [-0.4, 0.4]],  #smoothness\n",
    "                             dtype=tf.float32)\n",
    "    \n",
    "    numcol = tf.constant(17, name='numcol')    \n",
    "    i = tf.constant(0, name='i')\n",
    "    \n",
    "    new_table = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True, name='new_table')\n",
    "    \n",
    "    _, _, _, _, new_table = tf.while_loop(_normalise_condition, _normalise_body,\n",
    "                      [i, numcol, tf_MIN_MAX, table, new_table],\n",
    "                      shape_invariants=[i.get_shape(),\n",
    "                                        numcol.get_shape(),\n",
    "                                        tf_MIN_MAX.get_shape(),\n",
    "                                        table.get_shape(),\n",
    "                                        tf.TensorShape(None)])\n",
    "    \n",
    "    new_table = new_table.stack()\n",
    "    new_table = tf.reshape(new_table, (17,))\n",
    "\n",
    "    return new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == CLASS_NAMES\n",
    "\n",
    "#@tf.function\n",
    "def decode_image(byte_data):\n",
    "    #Get the image from the byte string\n",
    "    img = image_decode_fits(byte_data, 0) \n",
    "    img = tf.reshape(img, (128,128,1))\n",
    "    return img\n",
    "\n",
    "def decode_bintable(byte_data):\n",
    "    #Get the table from the byte string\n",
    "    morph = bintable_decode_fits(byte_data, 3)\n",
    "    morph = tf.reshape(morph, (53,))\n",
    "    morph = get_columns(morph)\n",
    "    morph = normalise_columns(morph)\n",
    "    \n",
    "    return morph\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    byte_data = tf.io.read_file(file_path)\n",
    "    img = decode_image(byte_data)\n",
    "    morph = decode_bintable(byte_data)\n",
    "    return img, morph, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "g = tf.random.Generator.from_seed(1)#int(time()))\n",
    "\n",
    "#@tf.function\n",
    "def augment_img(img, morph, label):\n",
    "    img = tf.image.rot90(img, k=g.uniform([], 0, 4, dtype=tf.int32))\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    \n",
    "    return img, morph, label\n",
    "\n",
    "#@tf.function\n",
    "def crop_img(img, morph, label):\n",
    "    img = tf.slice(img, [edge_cut,edge_cut,0], [IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    \n",
    "    return img, morph, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def prepare_dataset(ds, batch_size, shuffle_buffer_size=1000, training=False):\n",
    "    #Load images and labels\n",
    "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    #cache result\n",
    "    ds = ds.cache()\n",
    "    #shuffle images\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    \n",
    "    #Augment Image\n",
    "    if training:\n",
    "        ds = ds.map(augment_img, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(crop_img, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    #Set batches and repeat forever\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.repeat()\n",
    "    \n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_valid_ds = tf.data.Dataset.list_files(valid_path+'*/*'+extn)\n",
    "valid_ds = prepare_dataset(list_valid_ds, valid_image_count, valid_image_count)\n",
    "\n",
    "list_test_ds = tf.data.Dataset.list_files(test_path+'*/*'+extn)\n",
    "test_ds = prepare_dataset(list_test_ds, test_image_count, test_image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class morph_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(morph_model, self).__init__()\n",
    "        self.drop_rate = 0.2\n",
    "        \n",
    "        self.fuco1 = tf.keras.layers.Dense(128)\n",
    "        self.batn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop1 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.fuco4 = tf.keras.layers.Dense(128)\n",
    "        self.batn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop4 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        \n",
    "        x = self.fuco1(x)\n",
    "        x = self.batn1(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        \n",
    "        x = self.fuco4(x)\n",
    "        x = self.batn4(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop4(x, training=training)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(image_model, self).__init__()\n",
    "        self.drop_rate = 0.2\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 6, strides=1, padding='same')\n",
    "        self.batn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop1 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(2, 2, padding='same')\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, strides=1, padding='same')\n",
    "        self.batn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop2 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(2, 2, padding='same')\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, strides=1, padding='same')\n",
    "        self.batn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop3 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(128, 3, strides=1, padding='same')\n",
    "        self.batn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop4 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D(2, 2, padding='same')\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(2048) #2048\n",
    "        self.batn5 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop5 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc2 = tf.keras.layers.Dense(128)\n",
    "        self.batn6 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop6 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.batn1(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batn2(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batn3(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batn4(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop4(x, training=training)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.batn5(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop5(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batn6(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop6(x, training=training)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_morph_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(image_morph_model, self).__init__()\n",
    "        \n",
    "        self.image_model = image_model()\n",
    "        self.morph_model = morph_model()\n",
    "        \n",
    "        self.drop_rate = 0.2\n",
    "        \n",
    "        self.im_fuco1 = tf.keras.layers.Dense(256)\n",
    "        self.im_batn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.im_drop1 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.im_y_out = tf.keras.layers.Dense(NO_CLASS, activation='softmax')\n",
    "        \n",
    "    def call(self, image, morph, training=True):\n",
    "        img_latent = self.image_model(image, training)\n",
    "        mph_latent = self.morph_model(morph, training)\n",
    "        \n",
    "        x = tf.concat([img_latent, mph_latent], 1)\n",
    "        x = self.im_fuco1(x)\n",
    "        x = self.im_batn1(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.im_drop1(x)\n",
    "        \n",
    "        return self.im_y_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def val_step(images, morphs, labels):\n",
    "    '''labels should be one_hot'''\n",
    "    pred = model(images, morphs, training=False)\n",
    "    v_loss = total_loss(labels, pred)\n",
    "    mean_v_loss = tf.reduce_mean(v_loss)\n",
    "\n",
    "    #tf statistics tracking\n",
    "    val_loss(mean_v_loss)\n",
    "    val_accuracy(labels, pred)\n",
    "    return pred\n",
    "\n",
    "#@tf.function\n",
    "def tst_step(images, morphs, labels):\n",
    "    '''labels should be one_hot'''\n",
    "    pred = model(images, morphs, training=False)\n",
    "    t_loss = total_loss(labels, pred)\n",
    "    mean_t_loss = tf.reduce_mean(t_loss)\n",
    "\n",
    "    #tf statistics tracking\n",
    "    tst_loss(mean_t_loss)\n",
    "    tst_accuracy(labels, pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = image_morph_model()\n",
    "model.load_weights('./saved_model_HSC_SSP_wAS_128-sex-saved/checkpoint')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4) \n",
    "    \n",
    "total_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "tst_loss = tf.keras.metrics.Mean(name='tst_loss')\n",
    "tst_accuracy = tf.keras.metrics.CategoricalAccuracy(name='tst_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('To validate on', valid_image_count)\n",
    "\n",
    "template = 'Epoch {}\\nValid Loss: {:.3g}, Valid Accuracy: {:.3g}'\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    \n",
    "    #Validate  \n",
    "    print('Epoch', epoch+1, 'validation')\n",
    "    y_val_all = None\n",
    "    val_pred = None\n",
    "    for step in range(0, STEPS_PER_VALID_EPOCH):\n",
    "        i_val, m_val, y_val = next(iter(valid_ds))\n",
    "        if y_val_all is None:\n",
    "            y_val_all = y_val\n",
    "        else:\n",
    "            y_val_all = np.vstack((y_val_all, y_val))\n",
    "        pred = val_step(i_val, m_val, y_val)\n",
    "        if val_pred is None:\n",
    "            val_pred = pred\n",
    "        else:\n",
    "            val_pred = np.vstack((val_pred, pred))\n",
    "    \n",
    "    print(template.format(epoch+1, val_loss.result(), val_accuracy.result()))\n",
    "    \n",
    "    y_val = np.argmax(y_val_all, axis=1)\n",
    "    y_out_val_agm = np.argmax(val_pred, axis=1)\n",
    "    for j in range(0, NO_CLASS):\n",
    "        testing = np.where(y_out_val_agm == j)\n",
    "        correct = np.where(np.logical_and(y_out_val_agm == j, y_val == j))\n",
    "        validating = np.where(y_val == j)\n",
    "\n",
    "        if len(testing[0]) == 0:\n",
    "            cor_tes = 'inf'\n",
    "        elif len(correct[0]) == 0:\n",
    "            cor_tes = 0.0\n",
    "        else:\n",
    "            cor_tes = len(correct[0])/len(testing[0])\n",
    "            cor_tes = round(cor_tes, 3)\n",
    "\n",
    "        if len(validating[0]) == 0:\n",
    "            cor_val = 'inf'\n",
    "        elif len(correct[0]) == 0:\n",
    "            cor_val = 0.0\n",
    "        else:\n",
    "            cor_val = len(correct[0])/len(validating[0])\n",
    "            cor_val = round(cor_val, 3)\n",
    "\n",
    "        print('Val \\t Are', CLASS_NAMES[j], ' classed ', CLASS_NAMES[j], ':', cor_val, \n",
    "              '(',len(correct[0]), 'of', len(validating[0]),')')\n",
    "        print('Val \\t Classed', CLASS_NAMES[j], ' are ', CLASS_NAMES[j], ':', cor_tes, \n",
    "              '(',len(correct[0]), 'of', len(testing[0]),')')\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = np.arange(0.00, 1.01, 0.01)\n",
    "\n",
    "val_recall = []\n",
    "val_fall_out = []\n",
    "val_dist = []\n",
    "\n",
    "t_mgr = np.where(y_val_all.numpy()[:,0] == 1)[0]\n",
    "t_nmg = np.where(y_val_all.numpy()[:,0] == 0)[0]\n",
    "for cut in cuts:\n",
    "    mgr = np.where(val_pred.numpy()[:,0] > cut)[0]\n",
    "    nmg = np.where(val_pred.numpy()[:,0] <= cut)[0]\n",
    "    \n",
    "    tp = np.intersect1d(t_mgr, mgr)\n",
    "    fp = np.intersect1d(t_nmg, mgr)\n",
    "    tn = np.intersect1d(t_nmg, nmg)\n",
    "    fn = np.intersect1d(t_mgr, nmg)\n",
    "    \n",
    "    TP = len(tp)\n",
    "    FP = len(fp)\n",
    "    TN = len(tn)\n",
    "    FN = len(fn)\n",
    "    \n",
    "    if cut == 0.5:\n",
    "        print(round((TP+TN)/(TP+TN+FP+FN),3))\n",
    "        print(round(TP/(TP+FN),3), TP, (TP+FN))\n",
    "        print(round(TP/(TP+FP),3), TP, (TP+FP))\n",
    "        print(round(TN/(TN+FP),3), TN, (TN+FP))\n",
    "        print(round(TN/(TN+FN),3), TN, (TN+FN))\n",
    "    \n",
    "    val_recall.append(TP/(TP+FN))\n",
    "    val_fall_out.append(FP/(FP+TN))\n",
    "    \n",
    "    val_dist.append( np.sqrt(np.square(1-val_recall[-1])+np.square(val_fall_out[-1])) )\n",
    "    \n",
    "best_cut_idx = np.argmin(val_dist)\n",
    "best_cut = cuts[best_cut_idx]\n",
    "\n",
    "plt.plot(val_fall_out, val_recall)\n",
    "plt.plot(val_fall_out[best_cut_idx], val_recall[best_cut_idx], 'o')\n",
    "plt.xlabel('Fall Out')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "print('Best at', best_cut)\n",
    "\n",
    "mgr = np.where(val_pred.numpy()[:,0] > best_cut)[0]\n",
    "nmg = np.where(val_pred.numpy()[:,0] <= best_cut)[0]\n",
    "\n",
    "tp = np.intersect1d(t_mgr, mgr)\n",
    "fp = np.intersect1d(t_nmg, mgr)\n",
    "tn = np.intersect1d(t_nmg, nmg)\n",
    "fn = np.intersect1d(t_mgr, nmg)\n",
    "\n",
    "TP = len(tp)\n",
    "FP = len(fp)\n",
    "TN = len(tn)\n",
    "FN = len(fn)\n",
    "\n",
    "print('Recall:', round(TP/(TP+FN),3))\n",
    "print('Precision:', round(TP/(TP+FP),3))\n",
    "print('Specificity:', round(TN/(TN+FP),3))\n",
    "print('NPV:', round(TN/(TN+FN),3))\n",
    "print('Accuracy:', round((TP+TN)/(TP+FP+TN+FN),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('To test on', test_image_count)\n",
    "\n",
    "template = 'Epoch {}\\nTest Loss: {:.3g}, Test Accuracy: {:.3g}'\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    \n",
    "    #Validate  \n",
    "    print('Epoch', epoch+1, 'test')\n",
    "    y_tst_all = None\n",
    "    tst_pred = None\n",
    "    for step in range(0, STEPS_PER_TEST_EPOCH):\n",
    "        i_tst, m_tst, y_tst = next(iter(test_ds))\n",
    "        if y_tst_all is None:\n",
    "            y_tst_all = y_tst\n",
    "        else:\n",
    "            y_tst_all = np.vstack((y_tst_all, y_tst))\n",
    "        pred = tst_step(i_tst, m_tst, y_tst)\n",
    "        if tst_pred is None:\n",
    "            tst_pred = pred\n",
    "        else:\n",
    "            tst_pred = np.vstack((tst_pred, pred))\n",
    "    \n",
    "    print(template.format(epoch+1, tst_loss.result(), tst_accuracy.result()))\n",
    "    \n",
    "    y_tst = np.argmax(y_tst_all, axis=1)\n",
    "    y_out_tst_agm = np.argmax(tst_pred, axis=1)\n",
    "    for j in range(0, NO_CLASS):\n",
    "        testing = np.where(y_out_tst_agm == j)\n",
    "        correct = np.where(np.logical_and(y_out_tst_agm == j, y_tst == j))\n",
    "        validating = np.where(y_tst == j)\n",
    "\n",
    "        if len(testing[0]) == 0:\n",
    "            cor_tes = 'inf'\n",
    "        elif len(correct[0]) == 0:\n",
    "            cor_tes = 0.0\n",
    "        else:\n",
    "            cor_tes = len(correct[0])/len(testing[0])\n",
    "            cor_tes = round(cor_tes, 3)\n",
    "\n",
    "        if len(validating[0]) == 0:\n",
    "            cor_val = 'inf'\n",
    "        elif len(correct[0]) == 0:\n",
    "            cor_val = 0.0\n",
    "        else:\n",
    "            cor_val = len(correct[0])/len(validating[0])\n",
    "            cor_val = round(cor_val, 3)\n",
    "\n",
    "        print('Tst \\t Are', CLASS_NAMES[j], ' classed ', CLASS_NAMES[j], ':', cor_val, \n",
    "              '(',len(correct[0]), 'of', len(validating[0]),')')\n",
    "        print('Tst \\t Classed', CLASS_NAMES[j], ' are ', CLASS_NAMES[j], ':', cor_tes, \n",
    "              '(',len(correct[0]), 'of', len(testing[0]),')')\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best at', best_cut)\n",
    "\n",
    "t_mgr = np.where(y_tst_all.numpy()[:,0] == 1)[0]\n",
    "t_nmg = np.where(y_tst_all.numpy()[:,0] == 0)[0]\n",
    "\n",
    "mgr = np.where(tst_pred.numpy()[:,0] > best_cut)[0]\n",
    "nmg = np.where(tst_pred.numpy()[:,0] <= best_cut)[0]\n",
    "\n",
    "tp = np.intersect1d(t_mgr, mgr)\n",
    "fp = np.intersect1d(t_nmg, mgr)\n",
    "tn = np.intersect1d(t_nmg, nmg)\n",
    "fn = np.intersect1d(t_mgr, nmg)\n",
    "\n",
    "TP = len(tp)\n",
    "FP = len(fp)\n",
    "TN = len(tn)\n",
    "FN = len(fn)\n",
    "\n",
    "print('Recall:', round(TP/(TP+FN),3))\n",
    "print('Precision:', round(TP/(TP+FP),3))\n",
    "print('Specificity:', round(TN/(TN+FP),3))\n",
    "print('NPV:', round(TN/(TN+FN),3))\n",
    "print('Accuracy:', round((TP+TN)/(TP+FP+TN+FN),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_recall = []\n",
    "tst_fall_out = []\n",
    "tst_dist = []\n",
    "\n",
    "t_mgr = np.where(y_tst_all.numpy()[:,0] == 1)[0]\n",
    "t_nmg = np.where(y_tst_all.numpy()[:,0] == 0)[0]\n",
    "for cut in cuts:\n",
    "    mgr = np.where(tst_pred.numpy()[:,0] > cut)[0]\n",
    "    nmg = np.where(tst_pred.numpy()[:,0] <= cut)[0]\n",
    "    \n",
    "    tp = np.intersect1d(t_mgr, mgr)\n",
    "    fp = np.intersect1d(t_nmg, mgr)\n",
    "    tn = np.intersect1d(t_nmg, nmg)\n",
    "    fn = np.intersect1d(t_mgr, nmg)\n",
    "    \n",
    "    TP = len(tp)\n",
    "    FP = len(fp)\n",
    "    TN = len(tn)\n",
    "    FN = len(fn)\n",
    "    \n",
    "    if cut == 0.5:\n",
    "        print(round((TP+TN)/(TP+TN+FP+FN),3))\n",
    "        print(round(TP/(TP+FN),3), TP, (TP+FN))\n",
    "        print(round(TP/(TP+FP),3), TP, (TP+FP))\n",
    "        print(round(TN/(TN+FP),3), TN, (TN+FP))\n",
    "        print(round(TN/(TN+FN),3), TN, (TN+FN))\n",
    "    \n",
    "    tst_recall.append(TP/(TP+FN))\n",
    "    tst_fall_out.append(FP/(FP+TN))\n",
    "    \n",
    "    tst_dist.append( np.sqrt(np.square(1-tst_recall[-1])+np.square(tst_fall_out[-1])) )\n",
    "    \n",
    "better_cut_idx = np.argmin(tst_dist)\n",
    "better_cut = cuts[better_cut_idx]\n",
    "\n",
    "plt.plot(tst_fall_out, tst_recall)\n",
    "plt.plot(tst_fall_out[better_cut_idx], tst_recall[better_cut_idx], 'o')\n",
    "plt.xlabel('Fall Out')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "print('Better at', better_cut)\n",
    "\n",
    "mgr = np.where(tst_pred.numpy()[:,0] > better_cut)[0]\n",
    "nmg = np.where(tst_pred.numpy()[:,0] <= better_cut)[0]\n",
    "\n",
    "tp = np.intersect1d(t_mgr, mgr)\n",
    "fp = np.intersect1d(t_nmg, mgr)\n",
    "tn = np.intersect1d(t_nmg, nmg)\n",
    "fn = np.intersect1d(t_mgr, nmg)\n",
    "\n",
    "TP = len(tp)\n",
    "FP = len(fp)\n",
    "TN = len(tn)\n",
    "FN = len(fn)\n",
    "\n",
    "print('Recall:', round(TP/(TP+FN),3))\n",
    "print('Precision:', round(TP/(TP+FP),3))\n",
    "print('Specificity:', round(TN/(TN+FP),3))\n",
    "print('NPV:', round(TN/(TN+FN),3))\n",
    "print('Accuracy:', round((TP+TN)/(TP+FP+TN+FN),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(val_fall_out)\n",
    "val_fall_out_sort = np.array(val_fall_out)[order]\n",
    "val_recall_sort = np.array(val_recall)[order]\n",
    "val_auc = np.trapz(val_recall_sort, val_fall_out_sort)\n",
    "\n",
    "order = np.argsort(tst_fall_out)\n",
    "tst_fall_out_sort = np.array(tst_fall_out)[order]\n",
    "tst_recall_sort = np.array(tst_recall)[order]\n",
    "tst_auc = np.trapz(tst_recall_sort, tst_fall_out_sort)\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, 1*(5.5*2)/3.0))\n",
    "ax = plt.axes([1,1,1,1])\n",
    "ax.plot([0.0, 1.0],[0.0, 1.0], ':', label='Random Network', c='r', lw=2)\n",
    "ax.plot(val_fall_out, val_recall, label='Validation', c='b', lw=2)\n",
    "ax.plot(tst_fall_out, tst_recall, label='Testing', c='g', lw=2)\n",
    "\n",
    "ax.text(0.7, 0.35, ' Val AUC: '+str(round(val_auc,3)), fontsize=14)\n",
    "ax.text(0.7, 0.27, 'Test AUC: '+str(round(tst_auc,3)), fontsize=14)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(axis='x', which='major', direction='in', top=True)\n",
    "ax.tick_params(axis='y', which='major', direction='in', right=True)\n",
    "ax.set_xlabel('Fall Out', fontsize=16)\n",
    "ax.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "ax.legend(loc=0, frameon=False, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
