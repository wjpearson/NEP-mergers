{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "from astropy.table import Table\n",
    "from astropy.cosmology import Planck15 as cosmo\n",
    "from math import pi\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if GPUs. If there are, some code to fix cuDNN bugs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'HSC_SSP-morphology-sex-train.fits'\n",
    "valid_file = 'HSC_SSP-morphology-sex-valid.fits'\n",
    "\n",
    "MIN_MAX = {'asymmetry':[-4.0,4.0],\n",
    "           'concentration':[0.0, 6.0],\n",
    "           'deviation':[0.0, 3.0],\n",
    "           'ellipticity_asymmetry':[0.0, 1.0],\n",
    "           'ellipticity_centroid':[0.0, 1.0],\n",
    "           'elongation_asymmetry':[1.0, 8.0],\n",
    "           'elongation_centroid':[1.0, 8.0],\n",
    "           'gini':[0.0, 1.0],\n",
    "           'gini_m20_bulge':[-3, 3],\n",
    "           'gini_m20_merger':[-1.0, 1.0],\n",
    "           'intensity':[0.0, 1.0],\n",
    "           'm20':[-4.0, 0.0],\n",
    "           'multimode':[0.0, 1.0],\n",
    "           'sersic_amplitude':[0.0, 200.0],\n",
    "           'sersic_ellip':[-6.0, 3.0],\n",
    "           'sersic_n':[0.0, 50.0],\n",
    "           'smoothness':[-0.4,0.4]}\n",
    "\n",
    "CLASS_NAMES = ['merger', 'nonmerger']\n",
    "NO_CLASS = len(CLASS_NAMES)\n",
    "CLASS_COLUMN = 'class'\n",
    "\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "\n",
    "STEPS_PER_EPOCH = 1\n",
    "STEPS_PER_VALID_EPOCH = 1\n",
    "\n",
    "train_photo_count = 0\n",
    "valid_photo_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fits_dataset(file_path, class_column, class_number, min_max, use_columns=None):\n",
    "    table = Table.read(file_path)\n",
    "    \n",
    "    if 'train' in file_path:\n",
    "        global train_photo_count\n",
    "        global STEPS_PER_EPOCH\n",
    "        train_photo_count = len(table)\n",
    "        STEPS_PER_EPOCH = np.ceil(train_photo_count/BATCH_SIZE).astype(int)\n",
    "        batch_size = BATCH_SIZE\n",
    "    elif 'valid' in file_path:\n",
    "        global valid_photo_count\n",
    "        global STEPS_PER_VALID_EPOCH\n",
    "        valid_photo_count = len(table)\n",
    "        STEPS_PER_VALID_EPOCH = 1\n",
    "        batch_size = valid_photo_count\n",
    "    \n",
    "    #labels\n",
    "    labels = tf.one_hot(table[class_column].data, class_number)\n",
    "    \n",
    "    #data\n",
    "    data = []\n",
    "    nan = []\n",
    "    for column in min_max.keys():\n",
    "        data.append(table[column].data.astype(np.float32))\n",
    "        nan.append(np.where(data[-1] == -99))\n",
    "        data[-1] -= min_max[column][0]\n",
    "        data[-1] /= (min_max[column][1] - min_max[column][0])\n",
    "    data = np.array(data)\n",
    "    data = data.T\n",
    "    \n",
    "    for i in range(0, len(nan)):\n",
    "        data[nan[i]] = -1.0\n",
    "        \n",
    "    if use_columns is not None:\n",
    "        #photometric data\n",
    "        data2 = []\n",
    "        for column in use_columns:\n",
    "            data2.append(table[column].data)\n",
    "        data2 = np.array(data2)  \n",
    "        data2 = data2.T\n",
    "\n",
    "        nan = np.where(~np.isfinite(data2))\n",
    "\n",
    "        for i in range(0, len(data2)):\n",
    "            data2[i] -= np.nanmin(data2[i])\n",
    "            data2[i] /= np.nanmax(data2[i])\n",
    "\n",
    "        if len(nan[0]) > 0:\n",
    "            data2[nan] = -1.0\n",
    "            \n",
    "        data = np.hstack((data, data2))\n",
    "        \n",
    "    ds = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    ds = ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare_fits_dataset(train_file, CLASS_COLUMN, NO_CLASS, MIN_MAX)\n",
    "valid_ds = prepare_fits_dataset(valid_file, CLASS_COLUMN, NO_CLASS, MIN_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class morph_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(morph_model, self).__init__()\n",
    "        self.drop_rate = 0.2\n",
    "        \n",
    "        self.fuco1 = tf.keras.layers.Dense(128)\n",
    "        self.batn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop1 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.fuco4 = tf.keras.layers.Dense(128)\n",
    "        self.batn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop4 = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        \n",
    "        x = self.fuco1(x)\n",
    "        x = self.batn1(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        \n",
    "        x = self.fuco4(x)\n",
    "        x = self.batn4(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop4(x, training=training)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class morph_wrapper(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(morph_wrapper, self).__init__()\n",
    "        self.y_out = tf.keras.layers.Dense(NO_CLASS, activation='softmax')\n",
    "        \n",
    "        self.morph_model = morph_model()\n",
    "        \n",
    "    def call(self, morph, training=True):\n",
    "        x = self.morph_model(morph, training)\n",
    "        return self.y_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(data, labels):\n",
    "    '''labels shoule be one_hot'''\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(data)\n",
    "        loss = total_loss(labels, pred)\n",
    "        mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    #Update gradients and optimize\n",
    "    grads = tape.gradient(mean_loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    #tf statistics tracking\n",
    "    train_loss(mean_loss)\n",
    "    train_accuracy(labels, pred)\n",
    "\n",
    "@tf.function\n",
    "def val_step(data, labels):\n",
    "    '''labels should be one_hot'''\n",
    "    pred = model(data, training=False)\n",
    "    v_loss = total_loss(labels, pred)\n",
    "    mean_v_loss = tf.reduce_mean(v_loss)\n",
    "\n",
    "    #tf statistics tracking\n",
    "    val_loss(mean_v_loss)\n",
    "    val_accuracy(labels, pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = morph_wrapper()\n",
    "\n",
    "total_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('To train on', train_photo_count)\n",
    "print('To validate on', valid_photo_count)\n",
    "peak = [0, 0, 100]\n",
    "\n",
    "t_los = []\n",
    "t_acc = []\n",
    "v_los = []\n",
    "v_acc = []\n",
    "\n",
    "template = 'Epoch {}\\nTrain Loss: {:.3g}, Train Accuracy: {:.3g}\\nValid Loss: {:.3g}, Valid Accuracy: {:.3g}'\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    \n",
    "    #Train\n",
    "    for step in range(0, STEPS_PER_EPOCH):\n",
    "        x_batch, y_batch = next(iter(train_ds))\n",
    "        train_step(x_batch, y_batch)\n",
    "    \n",
    "    #Validate  \n",
    "    y_val_all = None\n",
    "    val_pred = None\n",
    "    for step in range(0, STEPS_PER_VALID_EPOCH):\n",
    "        x_val, y_val = next(iter(valid_ds))\n",
    "        if y_val_all is None:\n",
    "            y_val_all = y_val\n",
    "        else:\n",
    "            y_val_all = np.vstack((y_val_all, y_val))\n",
    "        pred = val_step(x_val, y_val)\n",
    "        if val_pred is None:\n",
    "            val_pred = pred\n",
    "        else:\n",
    "            val_pred = np.vstack((val_pred, pred))\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(template.format(epoch+1,\n",
    "                              train_loss.result(), train_accuracy.result(),\n",
    "                              val_loss.result(), val_accuracy.result()))\n",
    "    \n",
    "    t_los.append(train_loss.result())\n",
    "    t_acc.append(train_accuracy.result())\n",
    "    v_los.append(val_loss.result())\n",
    "    v_acc.append(val_accuracy.result())\n",
    "    \n",
    "    if val_loss.result() <= peak[2] or\\\n",
    "      (val_loss.result() == peak[2] and val_accuracy.result() >= peak[1]):\n",
    "        peak[0] = epoch+1\n",
    "        peak[1] = val_accuracy.result()\n",
    "        peak[2] = val_loss.result()\n",
    "        model.morph_model.save_weights('./saved_morph_model_HSC_SSP_wAS-sex/checkpoint')\n",
    "    \n",
    "    if val_accuracy.result() > 1.1:\n",
    "        y_val = np.argmax(y_val_all, axis=1)\n",
    "        y_out_val_agm = np.argmax(val_pred, axis=1)\n",
    "        for j in range(0, NO_CLASS):\n",
    "            testing = np.where(y_out_val_agm == j)\n",
    "            correct = np.where(np.logical_and(y_out_val_agm == j, y_val == j))\n",
    "            validating = np.where(y_val == j)\n",
    "     \n",
    "            if len(testing[0]) == 0:\n",
    "                cor_tes = 'inf'\n",
    "            elif len(correct[0]) == 0:\n",
    "                cor_tes = 0.0\n",
    "            else:\n",
    "                cor_tes = len(correct[0])/len(testing[0])\n",
    "                cor_tes = round(cor_tes, 3)\n",
    "    \n",
    "            if len(validating[0]) == 0:\n",
    "                cor_val = 'inf'\n",
    "            elif len(correct[0]) == 0:\n",
    "                cor_val = 0.0\n",
    "            else:\n",
    "                cor_val = len(correct[0])/len(validating[0])\n",
    "                cor_val = round(cor_val, 3)\n",
    "                \n",
    "            print('Val \\t Are', CLASS_NAMES[j], ' classed ', CLASS_NAMES[j], ':', cor_val, \n",
    "                  '(',len(correct[0]), 'of', len(validating[0]),')')\n",
    "            print('Val \\t Classed', CLASS_NAMES[j], ' are ', CLASS_NAMES[j], ':', cor_tes, \n",
    "                  '(',len(correct[0]), 'of', len(testing[0]),')')\n",
    "        print()\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print()\n",
    "print('Peaks at Epoch', peak[0], 'with accuracy', np.round(peak[1],3), 'and loss', np.round(peak[2],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
